{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import json\n",
    "from pprint import pprint\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_q = pd.read_csv(\"KR\\\\KR\\\\qid.csv\", names=[\"ID\", \"query\"])\n",
    "df_u = pd.read_csv(\"KR\\\\KR\\\\urlid.csv\", names=[\"ID\", \"URL\"], quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_u = df_u.transform({ \"ID\": (lambda x: x), \"URL\": (lambda x: \"https://ru.wikipedia.org\" + x)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение обхода из файла "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"ru_wikipedia\\\\full_out.json\") as f:\n",
    "    documents_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1413]\n",
      "https://ru.wikipedia.org/wiki/%D0%A0%D1%83%D1%81%D1%8C_(%D0%B3%D0%BE%D1%81%D1%83%D0%B4%D0%B0%D1%80%D1%81%D1%82%D0%B2%D0%BE)\n"
     ]
    }
   ],
   "source": [
    "ttt = df_u.loc[df_u['URL'] == df_u[\"URL\"][1413]][\"ID\"]\n",
    "print ttt.values\n",
    "\n",
    "print df_u[\"URL\"][1413]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizedDocument(object):\n",
    "    def __init__(self, doc_id, url, text, text_normed, list_order):\n",
    "        self.id = doc_id\n",
    "        self.url = url\n",
    "        self.text = text\n",
    "        self.text_normed = text_normed\n",
    "        self.list_order = list_order\n",
    "    \n",
    "    def __unicode__(self):\n",
    "        return u\"doc: id={}, url={}, snippet={}\".format(self.id, self.url, self.text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__unicode__().encode('utf-8')\n",
    "    \n",
    "    def document_normed_length(self):\n",
    "        return len(self.text_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizedQuery(object):\n",
    "    def __init__(self, query_id, query, query_normed):\n",
    "        self.id = query_id\n",
    "        self.query = query\n",
    "        self.query_normed = query_normed\n",
    "        \n",
    "    def __unicode__(self):\n",
    "        return u\"query: id={}, text={}\".format(self.id, self.query) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__unicode__().encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords = stopwords.words('russian') + list(string.punctuation) + [u\"википедия\"]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = RussianStemmer()\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens if w not in stopwords]    \n",
    "    return stemmed_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация документов и запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for i, d in enumerate(documents_json):\n",
    "    snippet = d[\"title\"].lower()\n",
    "    doc_id = df_u.loc[df_u['URL'] == d[\"url\"]][\"ID\"].values[0]\n",
    "    if not doc_id % 500:\n",
    "        print doc_id\n",
    "        \n",
    "    doc = NormalizedDocument(doc_id, d[\"url\"], unidecode.unidecode(snippet), \n",
    "                             [unidecode.unidecode(x) for x in normalize(snippet)], i)\n",
    "    documents.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "QUERIES_COUNT = len(df_q)\n",
    "\n",
    "for i in range(QUERIES_COUNT):\n",
    "    query_contents = unicode(df_q.loc[df_q[\"ID\"] == i][\"query\"].values[0], \"utf-8\")\n",
    "    \n",
    "    query = NormalizedQuery(i, unidecode.unidecode(query_contents),\n",
    "                            [unidecode.unidecode(x) for x in normalize(query_contents)])\n",
    "    queries.append(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инвертированный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InvIndex(object):\n",
    "    def __init__(self, words_lists, total_docs):\n",
    "        self.inv_index_and_frq = dict() # {word: [{doc_order: doc_frq, ...}, global_frequency], ...}\n",
    "        self.total_docs = total_docs\n",
    "        \n",
    "        self._build_inv_index(words_lists)\n",
    "        \n",
    "    def get_words(self):\n",
    "        return self.inv_index_and_frq.keys()\n",
    "    \n",
    "    def get_word_stats(self, w):\n",
    "        if w not in self.inv_index_and_frq:\n",
    "            return None\n",
    "        return self.inv_index_and_frq[w]\n",
    "    \n",
    "    def get_word_documents(self, w):\n",
    "        if w not in self.inv_index_and_frq:\n",
    "            return []\n",
    "        return self.inv_index_and_frq[w][0].keys()\n",
    "    \n",
    "    def get_word_frequency(self, w):\n",
    "        if w not in self.inv_index_and_frq:\n",
    "            return 0\n",
    "        return self.inv_index_and_frq[w][1]\n",
    "    \n",
    "    def get_word_frequency_in_doc(self, w, doc_order):\n",
    "        if w not in self.inv_index_and_frq:\n",
    "            return 0\n",
    "        if doc_order not in self.inv_index_and_frq[w][0]:\n",
    "            return 0\n",
    "        return self.inv_index_and_frq[w][0][doc_order]\n",
    "        \n",
    "\n",
    "    def _build_inv_index(self, words_lists):\n",
    "        for w_list_ind, w_list in enumerate(words_lists):\n",
    "            for w in w_list:\n",
    "                if w not in self.inv_index_and_frq:\n",
    "                    self.inv_index_and_frq[w] = [dict(), 0]\n",
    "                \n",
    "                if w_list_ind not in self.get_word_documents(w):\n",
    "                    self.inv_index_and_frq[w][0][w_list_ind] = 0\n",
    "                    self.inv_index_and_frq[w][1] = self.inv_index_and_frq[w][1] + 1\n",
    "                self.inv_index_and_frq[w][0][w_list_ind] = self.inv_index_and_frq[w][0][w_list_ind] + 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_index :\n",
      " amount of keys: 12491\n"
     ]
    }
   ],
   "source": [
    "inv_index = InvIndex([d.text_normed for d in documents], len(documents))\n",
    "\n",
    "# pprint(documents)\n",
    "\n",
    "# for iw, iw_stat in inv_index.inv_index_and_frq.iteritems():\n",
    "#     print iw, iw_stat\n",
    "print \"inv_index :\\n amount of keys: {}\".format(len(inv_index.get_words()))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "class RsvParams(object):\n",
    "    def __init__(self, b, k1, k2, avg_doc_len):\n",
    "        self.B = b\n",
    "        self.K1 = k1\n",
    "        self.K2 = k2\n",
    "        self.avg_doc_len = avg_doc_len      \n",
    "        \n",
    "        \n",
    "class Searcher(object):\n",
    "    def __init__(self, rsv_params, inv_index, idf_counter, total_docs):\n",
    "        self.rsv_params = rsv_params\n",
    "        self.inv_index = inv_index\n",
    "        self.search_result = []\n",
    "        self.idf_counter = idf_counter\n",
    "        self.total_docs = total_docs\n",
    "    \n",
    "    def _match_query(self, query, inv_index):\n",
    "        return {k: v for k, v in inv_index.inv_index_and_frq.iteritems() if k in query.query_normed}\n",
    "       \n",
    "    def _retrieve_docs_from_matched_index(self, matched_index, docs_collection):  \n",
    "        doc_orders = set()\n",
    "        for w in matched_index:\n",
    "            doc_orders.update(matched_index[w][0].keys())\n",
    "            \n",
    "        actual_docs = []\n",
    "        for i in list(doc_orders):\n",
    "            actual_docs.append(docs_collection[i])\n",
    "        \n",
    "        return actual_docs\n",
    "    \n",
    "    def _find_docs_via_query(self, documents_param, query):\n",
    "        matched_index = self._match_query(query, self.inv_index)\n",
    "\n",
    "        retr_d = self._retrieve_docs_from_matched_index(matched_index, documents_param)\n",
    "        return retr_d\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "    def calculate_rsv_for_docs_and_queries(self, docs, queries_param, norm_prop_name, is_normed = False):\n",
    "        del self.search_result[:]\n",
    "\n",
    "        for q in queries_param:\n",
    "            docs_rsv_list = self.calc_rsv_BM25(docs, q, norm_prop_name, is_normed)\n",
    "            self.search_result.append((q.id, docs_rsv_list))\n",
    "       \n",
    "    def calc_rsv_BM25(self, docs, q, norm_prop_name, is_normed = False):\n",
    "        docs_rsv_list = []\n",
    "        docs_found = self._find_docs_via_query(docs, q)\n",
    "        for d in docs_found:\n",
    "            rsv = self._rsv(q.query_normed, d.list_order, getattr(d, norm_prop_name), len(docs), self.inv_index, is_normed)\n",
    "            docs_rsv_list.append((d.id, rsv))\n",
    "        \n",
    "        return docs_rsv_list\n",
    "                \n",
    "    def _rsv(self, query_tokens, doc_order, doc_tokens, total_docs, inv_index, is_normed=False):\n",
    "        rsv_value = 0\n",
    "        for token in query_tokens:\n",
    "            idf_value = self.idf_counter(token, total_docs, inv_index)\n",
    "            \n",
    "            tf_value = self._tf(token, doc_order, len(doc_tokens), inv_index)\n",
    "            \n",
    "            rsv_value = rsv_value + idf_value * tf_value\n",
    "\n",
    "        if is_normed:\n",
    "            idf_sum = sum([self.idf_counter(token, total_docs, inv_index) for token in query_tokens])\n",
    "            rsv_value = float(rsv_value) / idf_sum\n",
    "\n",
    "        \n",
    "        return rsv_value\n",
    "\n",
    "    def _tf(self, token, doc_order, doc_len, inv_index):\n",
    "        ftd = inv_index.get_word_frequency_in_doc(token, doc_order)\n",
    "        tf_ret = float(ftd * (self.rsv_params.K1 + 1)) / (\n",
    "            ftd + self.rsv_params.K1 * (1 - self.rsv_params.B + self.rsv_params.B * doc_len / self.rsv_params.avg_doc_len))\n",
    "        return tf_ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "MAX_RELEVANT = 3\n",
    "avg_doc_len = float(sum([len(d.text_normed) for d in documents])) / len(documents)\n",
    "B = 0.75\n",
    "K1 = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_1(token, total_docs, inv_index):\n",
    "    N = len(documents)\n",
    "    Nt = inv_index.get_word_frequency(token)\n",
    "    idf_ret = math.log10(1 + float(N - Nt + 0.5) / (Nt + 0.5))\n",
    "    return idf_ret\n",
    "\n",
    "def idf_2(token, total_docs, inv_index):\n",
    "    N = len(documents)\n",
    "    Nt = inv_index.get_word_frequency(token)\n",
    "    idf_ret = math.log10(float(N + 0.5) / (Nt + 0.5))\n",
    "    return idf_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_relevant(queries_docs_rvs):\n",
    "    relevant_docs = []\n",
    "    for q in queries_docs_rvs:\n",
    "        relevant_docs.append((q[0], sorted(\n",
    "            q[1], key=lambda item: item[1], reverse=True)[:MAX_RELEVANT]))\n",
    "        \n",
    "    return relevant_docs\n",
    "\n",
    "        \n",
    "    \n",
    "def search_stats(b_p, k1_p, k2_p, inv_index_p, avg_doc_len, stats, idf_counter, norm_prop_name, is_normed=False):\n",
    "    rsv_parameters = RsvParams(b_p, k1_p, k2_p, avg_doc_len)\n",
    "    searcher = Searcher(rsv_parameters, inv_index_p, idf_counter, len(documents))\n",
    "\n",
    "    searcher.calculate_rsv_for_docs_and_queries(documents, queries, norm_prop_name, is_normed) \n",
    "    \n",
    "    relevant_docs = select_relevant(searcher.search_result)\n",
    "    \n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relev_docs = search_stats(B, K1, 0, inv_index, avg_doc_len, [], idf_1, \"text_normed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на train (+ запись в файлы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 query: id=17, text=vko [(7383, 5.361703490639291)]\n",
      "19 query: id=19, text=orkhideia vikipediia [(12412, 4.152411500006488), (16399, 4.152411500006488)]\n",
      "26 query: id=26, text=bendi vikipediia [(9352, 3.719409937636851)]\n",
      "40 query: id=40, text=torrent vikipediia [(9151, 5.361703490639291)]\n",
      "49 query: id=49, text=indiuk vikipediia [(1896, 4.152411500006488), (1295, 4.152411500006488)]\n",
      "54 query: id=54, text=iuar vikipediia [(11296, 4.152411500006488), (3714, 3.5164682174490305)]\n",
      "59 query: id=59, text=gol'shtein [(7491, 5.361703490639291)]\n",
      "62 query: id=62, text=kamaz vikipediia [(9711, 5.361703490639291)]\n",
      "83 query: id=83, text=wikipedia []\n",
      "124 query: id=124, text=boing 747 vikipediia [(5537, 4.152411500006488), (4435, 2.6919285943066744)]\n",
      "130 query: id=130, text=klubnika vikipediia [(13290, 5.069153503471511), (12205, 5.069153503471511)]\n",
      "158 query: id=158, text=aviakatastrofy vikipediia []\n",
      "188 query: id=188, text=zaiats vikipediia [(7118, 4.152411500006488), (3782, 4.152411500006488)]\n",
      "218 query: id=218, text=politekh [(1160, 5.361703490639291)]\n",
      "225 query: id=225, text=vshi vikipediia [(13892, 5.069153503471511), (1539, 4.152411500006488)]\n",
      "231 query: id=231, text=dnr vikipediia [(16611, 3.719409937636851)]\n",
      "237 query: id=237, text=torrent vikipediia [(9151, 5.361703490639291)]\n",
      "263 query: id=263, text=rokset vikipediia []\n",
      "264 query: id=264, text=wiki []\n",
      "265 query: id=265, text=tramp tauer [(2496, 4.152411500006488), (1052, 2.6919285943066744)]\n",
      "291 query: id=291, text=britanik vikipediia [(14159, 4.392054653485727)]\n",
      "320 query: id=320, text=soika peresmeshnitsa vikipediia [(1210, 5.361703490639291)]\n",
      "327 query: id=327, text=viki [(3591, 5.069153503471511), (6438, 4.152411500006488)]\n",
      "340 query: id=340, text=stolitsa burkina faso vikipediia [(801, 3.5164682174490305), (15907, 3.5164682174490305)]\n",
      "351 query: id=351, text=pompeia vikipediia [(405, 5.361703490639291)]\n",
      "382 query: id=382, text=lesbi vikipediia []\n",
      "431 query: id=431, text=nagornyi karabakh vikipediia [(16727, 8.544466153492214), (9362, 3.5164682174490305)]\n",
      "434 query: id=434, text=kinofil'm vikipediia [(14486, 4.392054653485727)]\n",
      "443 query: id=443, text=bagdasarovy vikipediia [(9123, 3.719409937636851)]\n",
      "445 query: id=445, text=iunkers [(17487, 5.069153503471511), (6582, 4.152411500006488)]\n",
      "459 query: id=459, text=kyrgyzstan vikipediia [(954, 4.392054653485727)]\n",
      "488 query: id=488, text=koaly vikipediia [(15432, 5.361703490639291)]\n",
      "498 query: id=498, text=luaz vikipediia []\n",
      "527 query: id=527, text=suitsid vikipediia [(3369, 4.392054653485727)]\n",
      "543 query: id=543, text=burkina faso vikipediia []\n",
      "549 query: id=549, text=kraz vikipediia [(15280, 5.361703490639291)]\n",
      "564 query: id=564, text=pase vikipediia [(8580, 3.225433640489738)]\n",
      "577 query: id=577, text=kraz [(15280, 5.361703490639291)]\n",
      "591 query: id=591, text=su vikipediia [(6526, 5.361703490639291)]\n",
      "595 query: id=595, text=kitai vikipediia [(16073, 4.152411500006488), (10464, 4.152411500006488)]\n",
      "630 query: id=630, text=vikipediia osetrovye [(5370, 5.361703490639291)]\n",
      "638 query: id=638, text=gugl vikipediia []\n",
      "672 query: id=672, text=mekhmet sultan vikipediia [(3335, 3.719409937636851)]\n",
      "685 query: id=685, text=gippopotam vikipediia [(2276, 4.392054653485727)]\n",
      "693 query: id=693, text=ioshkar ola vikipediia [(11678, 4.392054653485727)]\n",
      "698 query: id=698, text=vikipediia []\n",
      "721 query: id=721, text=chekhoslovakiia vikipediia [(12207, 5.069153503471511), (16214, 4.152411500006488)]\n",
      "728 query: id=728, text=tiuz []\n",
      "755 query: id=755, text=aifon vikipediia []\n",
      "758 query: id=758, text=bosch vikipediia [(1214, 3.719409937636851)]\n",
      "762 query: id=762, text=bobior vikipediia [(10346, 4.392054653485727)]\n",
      "807 query: id=807, text=pravoslavie vikipediia [(2615, 5.069153503471511), (4297, 4.152411500006488)]\n",
      "831 query: id=831, text=frg vikipediia [(13394, 4.152411500006488), (14927, 3.5164682174490305)]\n",
      "840 query: id=840, text=rolls rois vikipediia []\n",
      "841 query: id=841, text=vdv [(9014, 2.3064657638833084)]\n",
      "844 query: id=844, text=wasp vikipediia [(12825, 5.361703490639291)]\n",
      "845 query: id=845, text=navi vikipediia [(10220, 5.361703490639291)]\n",
      "856 query: id=856, text=po []\n",
      "868 query: id=868, text=rne vikipediia [(11407, 5.361703490639291)]\n",
      "881 query: id=881, text=g [(15413, 3.5164682174490305), (4385, 3.5164682174490305)]\n",
      "890 query: id=890, text=evenkiia vikipediia [(9511, 5.069153503471511), (57, 4.152411500006488)]\n",
      "896 query: id=896, text=renessans vikipediia [(1933, 4.152411500006488), (13196, 4.152411500006488)]\n",
      "916 query: id=916, text=kalisto [(428, 4.392054653485727)]\n",
      "919 query: id=919, text=morro [(2282, 5.361703490639291)]\n",
      "968 query: id=968, text=opiata vikipediia []\n",
      "972 query: id=972, text=mail vikipediia [(34, 4.392054653485727)]\n",
      "992 query: id=992, text=bmd vikipediia []\n",
      "998 query: id=998, text=pgu [(11960, 5.361703490639291)]\n",
      "1031 query: id=1031, text=klint istvud vikipediia [(12496, 8.544466153492214), (16698, 5.069153503471511)]\n",
      "1032 query: id=1032, text=zaiats [(7118, 4.152411500006488), (3782, 4.152411500006488)]\n",
      "1044 query: id=1044, text=dizel' poezd [(14625, 4.392054653485727)]\n",
      "1066 query: id=1066, text=nindzia vikipediia [(13212, 5.069153503471511), (95, 4.152411500006488)]\n",
      "1076 query: id=1076, text=alladin vikipediia []\n",
      "1078 query: id=1078, text=rep vikipediia [(3715, 5.069153503471511), (11240, 4.152411500006488)]\n",
      "1085 query: id=1085, text=podruga vikipediia [(4451, 5.069153503471511), (5730, 4.152411500006488)]\n",
      "1096 query: id=1096, text=raptor vikipediia [(16962, 5.069153503471511), (5983, 3.5164682174490305)]\n",
      "1108 query: id=1108, text=liaz [(5461, 3.719409937636851)]\n",
      "1116 query: id=1116, text=bmp vikipediia [(2386, 4.152411500006488), (17244, 4.152411500006488)]\n",
      "1150 query: id=1150, text=ertugrul vikipediia [(12557, 4.152411500006488), (8464, 3.5164682174490305)]\n",
      "1207 query: id=1207, text=zil [(9923, 4.392054653485727)]\n",
      "1209 query: id=1209, text=egiptus vikipediia []\n",
      "1232 query: id=1232, text=vikitsitatnik [(2819, 5.361703490639291)]\n",
      "1241 query: id=1241, text=glee vikipediia [(1284, 5.069153503471511), (15384, 3.5164682174490305)]\n",
      "1244 query: id=1244, text=azbuka vikipediia [(14635, 5.069153503471511), (6018, 4.152411500006488)]\n",
      "1258 query: id=1258, text=ulan ude vikipediia []\n",
      "1263 query: id=1263, text=vikipedii [(3356, 4.152411500006488), (12262, 4.152411500006488)]\n",
      "1265 query: id=1265, text=birma vikipediia [(6021, 4.152411500006488), (13368, 4.152411500006488)]\n",
      "1289 query: id=1289, text=uaz vikipediia [(8087, 4.152411500006488), (11337, 4.152411500006488)]\n",
      "1306 query: id=1306, text=solovki vikipediia [(3400, 5.069153503471511), (7010, 4.152411500006488)]\n",
      "1331 query: id=1331, text=vaz vikipediia []\n",
      "1369 query: id=1369, text=glok [(15550, 4.152411500006488), (15688, 4.152411500006488)]\n",
      "1412 query: id=1412, text=orkhidei vikipediia []\n",
      "1413 query: id=1413, text=o []\n",
      "1428 query: id=1428, text=chto gde kogda vikipediia []\n",
      "1436 query: id=1436, text=kmz [(6104, 5.361703490639291)]\n",
      "1442 query: id=1442, text=tu 154 vikipediia []\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_submission.csv\", \"w\") as subm_f:\n",
    "    for rd in relev_docs[:TRAIN_SIZE]:\n",
    "        if len(rd[1]) != 3:\n",
    "            print rd[0], queries[rd[0]], rd[1]\n",
    "                        \n",
    "        subm_f.write(\"{},{},{},{}\\n\".format(rd[0],\n",
    "                                                rd[1][0][0] if len(rd[1]) >= 1 else 0,\n",
    "                                                rd[1][1][0] if len(rd[1]) >= 2 else 0,\n",
    "                                                rd[1][2][0] if len(rd[1]) >= 3 else 0))\n",
    "        \n",
    "with open(\"test_submission.csv\", \"w\") as subm_f:\n",
    "    for rd in relev_docs[TRAIN_SIZE:]:\n",
    "        if len(rd[1]) != 3:\n",
    "            print rd[0], queries[rd[0]], rd[1]\n",
    "                        \n",
    "        subm_f.write(\"{},{},{},{}\\n\".format(rd[0],\n",
    "                                                rd[1][0][0] if len(rd[1]) >= 1 else 0,\n",
    "                                                rd[1][1][0] if len(rd[1]) >= 2 else 0,\n",
    "                                                rd[1][2][0] if len(rd[1]) >= 3 else 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: id=992, text=bmd vikipediia\n"
     ]
    }
   ],
   "source": [
    "print queries[992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[doc: id=2035, url=https://ru.wikipedia.org/wiki/Ikarus, snippet=ikarus]\n",
      "ikarus\n"
     ]
    }
   ],
   "source": [
    "filtered = filter(lambda d: d.id in [2035], documents)\n",
    "print filtered\n",
    "for tn in filtered[0].text_normed:\n",
    "    print tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42761111111111111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_metrics import mapk\n",
    "\n",
    "def getList(filename):\n",
    "    with open(filename) as input:\n",
    "        return [line[:-1].split(',')[1:] for line in input]\n",
    "    \n",
    "mapk(getList('KR\\\\KR\\\\train_data.csv'), getList('train_submission.csv'),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
