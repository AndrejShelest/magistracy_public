{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return float(1) / (1 + math.exp(-x))        \n",
    "\n",
    "\n",
    "def logit(w, x, y, lambd):\n",
    "    return math.log(1 + math.exp(-1*y*np.dot(w, x))) + lambd * math.sqrt(sum([x ** 2 for x in w]))\n",
    "\n",
    "\n",
    "def logit_loss(w, samples, lambd):\n",
    "    return np.average([logit(w, s[:-1], s[-1], lambd) for s in samples]);\n",
    "                                                                         \n",
    "                                                                         \n",
    "def logit_loss_partial_deriv(w, samples, j, lambd):\n",
    "    norm = math.sqrt(sum([x ** 2 for x in w]))\n",
    "    tikhonov_deriv = (lambd * w[j] / norm) if norm > 0 else 0\n",
    "    return np.average([s[-1] * s[j] * (sigmoid(s[-1] * np.dot(w, s[:-1])) - 1) for s in samples]) + tikhonov_deriv\n",
    "\n",
    "\n",
    "def logit_loss_gradient(w, samples, lambd):\n",
    "    d = len(samples[0]) - 1\n",
    "    return [logit_loss_partial_deriv(w, samples, j, lambd) for j in xrange(d)]\n",
    "\n",
    "\n",
    "def logit_loss_one_var_gradient(w, samples, lambd, j):\n",
    "    one_sample = [samples[j]]\n",
    "    return logit_loss_gradient(w, one_sample, lambd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YES_LABEL = 1\n",
    "NO_LABEL = -1\n",
    "\n",
    "def read_data_from_file(filename):\n",
    "    samples = []\n",
    "    classes = set()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line_splitted = line.strip().split(\",\")\n",
    "            samples.append(([float(x) for x in line_splitted[:-1]], line_splitted[-1]))\n",
    "            classes.add(line_splitted[-1])\n",
    "    return samples, classes\n",
    "\n",
    "\n",
    "def mark_set(samples, class_label):\n",
    "    marked = []\n",
    "    for s in samples:\n",
    "        ts = [1] + list(s[0]) # make homogenous\n",
    "        ts.extend([YES_LABEL if s[1] == class_label else NO_LABEL])\n",
    "        marked.append(ts)\n",
    "    return marked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gd(samples, T, nu, lambd):\n",
    "    d = len(samples[0]) -1\n",
    "    w = np.zeros(d)\n",
    "    w_history = [w]\n",
    "    \n",
    "    for t in xrange(T):\n",
    "        vt = logit_loss_gradient(w, samples, lambd)\n",
    "        w = np.subtract(w, np.dot(nu, vt))\n",
    "        w_history.append(w)\n",
    "        \n",
    "    w_res = np.zeros(d)\n",
    "    for w_h in w_history:\n",
    "        w_res = np.add(w_res, w_h)\n",
    "        \n",
    "    return w_history[-1]\n",
    "\n",
    "def stochastic_gd(samples, T, nu, lambd):\n",
    "    d = len(samples[0]) - 1\n",
    "    w = np.zeros(d)\n",
    "    w_history = [w]\n",
    "    \n",
    "    for t in xrange(T):\n",
    "        vt = logit_loss_one_var_gradient(w, samples, lambd, random.randint(0, len(samples) - 1))\n",
    "        w = np.subtract(w, np.dot(nu, vt))\n",
    "        w_history.append(w)\n",
    "        \n",
    "    w_res = np.zeros(d)\n",
    "    for w_h in w_history:\n",
    "        w_res = np.add(w_res, w_h)\n",
    "        \n",
    "    return w_history[-1]\n",
    "\n",
    "\n",
    "def test_sample(x, y, w):\n",
    "    prediction = sigmoid(np.dot(w,x))\n",
    "    if (prediction >= 0.5 and y == YES_LABEL) or (prediction < 0.5 and y == NO_LABEL):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_samples(samples_for_test, w):\n",
    "    predicted = sum( [test_sample(x[:-1], x[-1], w) for x in samples_for_test])\n",
    "    return float(predicted) / len(samples_for_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_RANGE = xrange(25, 210, 50)\n",
    "LAMBDA_RANGE = np.arange(0.0001, 0.01, 0.004)\n",
    "NU_RANGE = np.arange(0.0001, 0.105, 0.005)\n",
    "T_RANGE_LAMBDA = xrange(25, 110, 25)\n",
    "LAMBDA_RANGE_BATCH = np.arange(0.0001, 0.01, 0.004)\n",
    "NU_RANGE_BATCH = np.arange(0.0001, 0.2001, 0.01)\n",
    "K = 10\n",
    "\n",
    "\n",
    "def k_fold(samples, k, t_range, lambda_range, nu_range, gd_func):\n",
    "    block_size = int(math.floor(float(len(samples)) / k))\n",
    "    \n",
    "    all_logit_loss_history = []\n",
    "\n",
    "    iters_passed = 0\n",
    "    \n",
    "    for T in t_range:\n",
    "        for l in lambda_range:\n",
    "            for nu in nu_range:              \n",
    "                \n",
    "                logit_loss_history = []\n",
    "                \n",
    "                for ind in xrange(0, len(samples), block_size):\n",
    "                    fold_test = samples[ind:min(ind+block_size, len(samples))]\n",
    "                    fold_train = [x for x in samples if x not in fold_test]            \n",
    "\n",
    "                    w_fold_trained = gd_func(fold_train, T, nu, l)\n",
    "                    log_loss = logit_loss(w_fold_trained, fold_test, l)\n",
    "                    logit_loss_history.append([T, l, nu, log_loss])\n",
    "                \n",
    "                avg_loss = np.average([loss[3] for loss in logit_loss_history])\n",
    "                all_logit_loss_history.append([T, l, nu, avg_loss])\n",
    "                \n",
    "                \n",
    "                print \"T=\", T, \",l=\", l, \",nu=\", nu, \"loss: \", avg_loss\n",
    "\n",
    "    params = all_logit_loss_history[np.argmin(np.array(all_logit_loss_history)[:, -1])][:-1]\n",
    "    return params[0], params[1], params[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, classes = read_data_from_file(\"iris.data\")\n",
    "\n",
    "stochastic_log_file = open('stochastic-k-fold.log', 'w')\n",
    "batch_log_file = open('batch-k-fold.log', 'w')\n",
    "\n",
    "\n",
    "for c in list(classes):\n",
    "    marked_data = mark_set(data, c)\n",
    "\n",
    "    np.random.shuffle(marked_data)\n",
    "    train_length = int(math.floor(len(marked_data) * 0.9))\n",
    "    train_data = marked_data[:train_length]\n",
    "    test_data = marked_data[train_length:]\n",
    "\n",
    "    sys.stdout = batch_log_file\n",
    "    print \"batch k-fold: \"\n",
    "    \n",
    "    T, lambd, nu = k_fold(train_data, K, T_RANGE_LAMBDA, LAMBDA_RANGE_BATCH, NU_RANGE_BATCH, batch_gd)\n",
    "    w_trained = batch_gd(train_data, T, nu, lambd)\n",
    "    predict = test_samples(test_data, w_trained)\n",
    "    \n",
    "    sys.stdout = batch_log_file\n",
    "    print \"batch: c=\", c, \", T=\", T, \", l=\", lambd, \", nu=\", nu\n",
    "    print \"batch: \", w_trained, predict\n",
    "    sys.stdout = sys.__stdout__ \n",
    "    print \"batch: c=\", c, \", T=\", T, \", l=\", lambd, \", nu=\", nu\n",
    "    print \"batch: \", w_trained, predict\n",
    "   \n",
    "\n",
    "\n",
    "    sys.stdout = stochastic_log_file\n",
    "    print \"stochastic k-fold: \"\n",
    "    \n",
    "    T_st, lambd_st, nu_st = k_fold(train_data, K, T_RANGE, LAMBDA_RANGE, NU_RANGE, stochastic_gd)\n",
    "    w_trained_st = stochastic_gd(train_data, T_st, nu_st, lambd_st)\n",
    "    predict_st = test_samples(test_data, w_trained_st)\n",
    "    \n",
    "    sys.stdout = stochastic_log_file\n",
    "    print \"stochastic: c=\", c, \", T=\", T_st, \", l=\", lambd_st, \", nu=\", nu_st\n",
    "    print \"stochastic: \", w_trained_st, predict_st\n",
    "    sys.stdout = sys.__stdout__\n",
    "    print \"stochastic: c=\", c, \", T=\", T_st, \", l=\", lambd_st, \", nu=\", nu_st\n",
    "    print \"stochastic: \", w_trained_st, predict_st\n",
    "\n",
    "stochastic_log_file.close();\n",
    "batch_log_file.close();\n",
    "\n",
    "sys.stdout = sys.__stdout__\n",
    "print \"FINISHED\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
